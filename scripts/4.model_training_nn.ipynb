{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb4319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# September 2025\n",
    "# Train multilingual classification neural network\n",
    "# Violeta Berdejo-Espinola\n",
    "\n",
    "# pytorch dataset\n",
    "# pythorch model\n",
    "# pytorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers sys matplotlib numpy sklearn pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('system version:', sys.version)\n",
    "print('pytorch version:', torch.__version__)\n",
    "print('numpy version:', np.version)\n",
    "\n",
    "import platform; \n",
    "\n",
    "print(f'mac processor: {platform.mac_ver()}')\n",
    "print(f'mps is available: {torch.backends.mps.is_built()}')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7226e87",
   "metadata": {},
   "source": [
    "# function to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "filepath_x = \"../data/for_analysis/eng_x.pickle\"\n",
    "\n",
    "filepath_y = \"../data/for_analysis/eng_y.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951efaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_x, path_y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path_x (list[str]): Input texts.\n",
    "        path_y (list[str]): Input labels.\n",
    "        \n",
    "    Returns:\n",
    "        lists\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path_x,'rb') as x_file:\n",
    "        texts = pickle.load(x_file)\n",
    "    with open (path_y, 'rb') as y_file:\n",
    "        labels = pickle.load(y_file)\n",
    "        \n",
    "    return texts[:500], labels[:500]\n",
    "        \n",
    "texts, labels = load_data(\"../data/for_analysis/eng_x.pickle\", \"../data/for_analysis/eng_y.pickle\")\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to load multilingual data\n",
    "\n",
    "# file paths\n",
    "\n",
    "# filepaths_x = [\n",
    "#     \"../data/for_analysis/eng_x.pickle\",\n",
    "#     # \"../data/for_analysis/jap_x.pickle\"\n",
    "#     # \"../data/for_analysis/spa_x.pickle\"\n",
    "# ]\n",
    "\n",
    "# filepaths_y = [\n",
    "#     \"../data/for_analysis/eng_y.pickle\",\n",
    "#     # \"../data/for_analysis/jap_y.pickle\"\n",
    "#     # \"../data/for_analysis/spa_y.pickle\"\n",
    "# ]\n",
    "\n",
    "# # def load_data(filepaths_x, filepaths_y):\n",
    "# #     \"\"\"Load and combine multilingual data\"\"\"\n",
    "# #     all_texts = []\n",
    "# #     all_labels = []\n",
    "    \n",
    "# #     for fp_x, fp_y in zip(filepaths_x, filepaths_y):\n",
    "# #         with open(fp_x, 'rb') as f:\n",
    "# #             texts = pickle.load(f)\n",
    "# #         with open(fp_y, 'rb') as f:\n",
    "# #             labels = pickle.load(f)\n",
    "            \n",
    "# #         all_texts.extend(texts)\n",
    "# #         all_labels.extend(labels)\n",
    "    \n",
    "# #     return all_texts[:40], all_labels[:40]\n",
    "\n",
    "\n",
    "# # texts, labels = load_data(filepaths_x, filepaths_y)\n",
    "\n",
    "# # print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38315b",
   "metadata": {},
   "source": [
    "# encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hf tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"       # max sequence length == 128 ~ 80 words\n",
    ")                                                                       # mapping to 768 dimensional vector space\n",
    "hf_model = AutoModel.from_pretrained(                                   # using a hf Tokenizer and Model function we can extend the max length\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "hf_model = hf_model.to(device)\n",
    "\n",
    "# function to encode text\n",
    "\n",
    "def encode_texts(texts, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Encode a list of texts into fixed-size embeddings using mean pooling.\n",
    "    \n",
    "    Args:\n",
    "        texts (list[str]): Input texts.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        model: Multilingual model (encoder).\n",
    "        device (torch.device): \"mpu\".\n",
    "        max_length (int): Maximum sequence length.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Forward pass (no gradients, eval mode)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "    # Mean pooling (ignore padding)\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1).float()  # (batch, seq_len, 1)\n",
    "    sum_embeddings = (token_embeddings * attention_mask).sum(dim=1)  # (batch, hidden_size)\n",
    "    sum_mask = attention_mask.sum(dim=1).clamp(min=1e-9)             # (batch, 1)\n",
    "    embeddings = sum_embeddings / sum_mask                           # (batch, hidden_size)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114b83b",
   "metadata": {},
   "source": [
    "# dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets class\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, model, device, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # create embedding for single text\n",
    "        embedding = encode_texts([text], self.tokenizer, self.model, self.device, self.max_length)\n",
    "        embedding = embedding.squeeze(0)  # remove batch dimension\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3780b",
   "metadata": {},
   "source": [
    "# data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with imbalance in loss function\n",
    "\n",
    "def compute_loss_weights(labels):\n",
    "    \"\"\"Compute class weights for weighted loss function\"\"\"\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    return torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze and visualize class distribution\n",
    "\n",
    "def analyze_class_distribution(labels):\n",
    "   \n",
    "    counter = Counter(labels)\n",
    "    print(\"Class distribution:\")\n",
    "    \n",
    "    for class_id, count in sorted(counter.items()):\n",
    "        \n",
    "        print(f\"Class {class_id}: {count} samples ({count/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    counts = list(counter.values())\n",
    "    imbalance_ratio = max(counts) / min(counts)\n",
    "    \n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader stratified batch sampler\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "class StratifiedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that creates stratified mini-batches where each batch \n",
    "    maintains approximately the same class distribution as the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, labels, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (list or array): Class labels for all samples\n",
    "            batch_size (int): Size of mini-batches\n",
    "            shuffle (bool): Whether to shuffle within each class\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Group indices by class\n",
    "        self.class_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            self.class_indices[label].append(idx)\n",
    "        \n",
    "        # Convert to lists\n",
    "        self.class_indices = {k: np.array(v) for k, v in self.class_indices.items()}\n",
    "        self.classes = list(self.class_indices.keys())\n",
    "        \n",
    "        # Calculate samples per class per batch\n",
    "        self.class_counts = {cls: len(indices) for cls, indices in self.class_indices.items()}\n",
    "        total_samples = sum(self.class_counts.values())\n",
    "        \n",
    "        # Proportional representation in each batch\n",
    "        self.samples_per_class = {\n",
    "            cls: max(1, int(batch_size * count / total_samples))\n",
    "            for cls, count in self.class_counts.items()\n",
    "        }\n",
    "        \n",
    "        # Adjust to ensure batch_size is maintained\n",
    "        diff = batch_size - sum(self.samples_per_class.values())\n",
    "        if diff != 0:\n",
    "            # Add/remove from largest class\n",
    "            largest_class = max(self.class_counts, key=self.class_counts.get)\n",
    "            self.samples_per_class[largest_class] += diff\n",
    "        \n",
    "        print(f\"Samples per class per batch: {self.samples_per_class}\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Shuffle indices within each class if required\n",
    "        if self.shuffle:\n",
    "            class_indices = {\n",
    "                cls: np.random.permutation(indices).tolist()\n",
    "                for cls, indices in self.class_indices.items()\n",
    "            }\n",
    "        else:\n",
    "            class_indices = {cls: indices.tolist() for cls, indices in self.class_indices.items()}\n",
    "        \n",
    "        # Create batches\n",
    "        batches = []\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        min_batches = min(\n",
    "            len(indices) // self.samples_per_class[cls]\n",
    "            for cls, indices in class_indices.items()\n",
    "        )\n",
    "        \n",
    "        for batch_idx in range(min_batches):\n",
    "            batch = []\n",
    "            for cls in self.classes:\n",
    "                start_idx = batch_idx * self.samples_per_class[cls]\n",
    "                end_idx = start_idx + self.samples_per_class[cls]\n",
    "                batch.extend(class_indices[cls][start_idx:end_idx])\n",
    "            \n",
    "            # Shuffle within batch\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(batch)\n",
    "            \n",
    "            batches.append(batch)\n",
    "        \n",
    "        # Handle remaining samples\n",
    "        remaining = []\n",
    "        for cls in self.classes:\n",
    "            start_idx = min_batches * self.samples_per_class[cls]\n",
    "            remaining.extend(class_indices[cls][start_idx:])\n",
    "        \n",
    "        # Create final partial batch if we have remaining samples\n",
    "        if len(remaining) >= self.batch_size // 2:  # Only if we have substantial samples\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(remaining)\n",
    "            batches.append(remaining[:self.batch_size])\n",
    "        \n",
    "        # Shuffle batch order\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batches)\n",
    "        \n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        min_batches = min(\n",
    "            len(indices) // self.samples_per_class[cls]\n",
    "            for cls, indices in self.class_indices.items()\n",
    "        )\n",
    "        return min_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weighted random sampler --> use instead of mini batches\n",
    "\n",
    "# def weighted_sampler(labels):\n",
    "#     \"\"\"Assign a weight inversely proportional to its class frequency\"\"\"\n",
    "    \n",
    "#     # 1. Create mapping of each class to a weight equal to 1 / count\n",
    "#     class_counts = Counter(labels)\n",
    "#     class_weights = {cls: 1.0/count for cls, count in class_counts.items()}\n",
    "#     print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "#     # 2. Replace class with that weight.\n",
    "#     sample_weights = [class_weights[label] for label in labels]\n",
    "#     print(len(sample_weights))\n",
    "    \n",
    "#     # 3. Sampler object\n",
    "#     sampler = WeightedRandomSampler(\n",
    "#         weights=sample_weights,\n",
    "#         num_samples=len(sample_weights),\n",
    "#         replacement=True # samples can be picked multiple times per epoch\n",
    "#     )\n",
    "    \n",
    "#     return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597e173",
   "metadata": {},
   "source": [
    "# classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier class\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=None, hidden_dims=[512, 512, 512], num_classes=2, dropout=0.2): \n",
    "        super(MLPClassifier, self).__init__()                                               \n",
    "        \"\"\"\n",
    "        Multi-layer perceptron for text classification.\n",
    "        \n",
    "        Args:\n",
    "        input_dim (feature vector): per sample and outputs\n",
    "        hidden_dim (list): hidden state\n",
    "        num_classes (int): classification classes\n",
    "        dropout (float): fraction of neurons to drop during training at each training step\n",
    "        \n",
    "        Returns: num_classes logits per sample\n",
    "        \"\"\"\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                \n",
    "                # a single fully connected layer -> performs the linear transformation on input data: 768 to 512\n",
    "                nn.Linear(prev_dim, hidden_dim), # it applies the operation: y = xW^T + b\n",
    "                                                 \n",
    "                # nn.BatchNorm1d(hidden_dim), # normalize inputs  - if dropout is used normalization is not \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        # container that organizes multiple layers into a pipeline\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1dab0",
   "metadata": {},
   "source": [
    "# train eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e64782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for embeddings, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass (no gradients)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass (update gradients)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Collect predictions and labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_tr = 100 * correct / total\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    recall_tr = recall_score(all_labels, all_preds)\n",
    "    f1_tr = f1_score(all_labels, all_preds)\n",
    "    precision_tr = precision_score(all_labels, all_preds)\n",
    "    \n",
    "    return (all_preds, all_labels, \n",
    "            avg_loss, recall_tr, precision_tr, f1_tr, accuracy_tr)\n",
    "\n",
    "# eval function \n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 3. Collect predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_val = 100 * correct / total\n",
    "    \n",
    "    # 5. Calculate metrics for each class\n",
    "    recall_val = recall_score(all_labels, all_preds)\n",
    "    f1_val = f1_score(all_labels, all_preds)\n",
    "    precision_val = precision_score(all_labels, all_preds)\n",
    "    \n",
    "    return (all_preds, all_labels, \n",
    "            avg_loss, recall_val, precision_val, f1_val, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bcef7",
   "metadata": {},
   "source": [
    "# load data, split, create datastes and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "texts, labels = load_data(filepath_x, filepath_y)\n",
    "\n",
    "print(f\"Total documents: {len(texts)}\")\n",
    "\n",
    "class_distribution = analyze_class_distribution(labels)\n",
    "\n",
    "weights = compute_loss_weights(labels)\n",
    "\n",
    "# split data       %% run it with different seeds %%\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=seed, stratify=labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=seed, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, \\\n",
    "      Val: {len(X_val)}, \\\n",
    "      Test: {len(X_test)}\")\n",
    "    \n",
    "# datasets\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, hf_model, device)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, hf_model, device)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer, hf_model, device)\n",
    "\n",
    "# weighted sampler\n",
    "\n",
    "# train_sampler, class_weights = weighted_sampler(y_train)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_batch_sampler = StratifiedBatchSampler(\n",
    "    labels=y_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_batch_sampler, num_workers=0)  # batch_sampler\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# get embedding dimension\n",
    "\n",
    "sample_embedding, _ = train_dataset[0]\n",
    "embedding_dim = sample_embedding.shape[0]\n",
    "\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166ffe3",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_loop():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MLPClassifier(input_dim=embedding_dim).to(device)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    \n",
    "    num_epochs = 25                       \n",
    "    patience = 5  # stop if no improvement after 5 epochs\n",
    "    patience_counter = 0\n",
    "    best_val_f1 = 0 \n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.05)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    train_preds, train_labels = [],[]\n",
    "    val_preds, val_labels = [],[]\n",
    "    train_losses, val_losses = [],[]\n",
    "    train_accs, val_accs = [],[]\n",
    "    train_recall, val_recall = [],[]\n",
    "    train_f1, val_f1 = [],[]\n",
    "    train_precision, val_precision = [],[]\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 1. Train\n",
    "        (train_pred, train_label, train_loss, tr_recall, tr_precision, tr_f1, train_acc) = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # 2. Validate\n",
    "        (val_pred, val_label, val_loss, v_recall, v_f1, v_precision, val_acc) = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Train Recall: {tr_recall:.4f}\")\n",
    "        print(f\"Train F1: {tr_f1:.4f}\")\n",
    "        print(f\"Train Precision: {tr_precision:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Val Recall: {v_recall:.4f}\")\n",
    "        print(f\"Val F1: {v_f1:.4f}\")\n",
    "        print(f\"Val Precision: {v_precision:.4f}\")\n",
    "        \n",
    "        # 3. Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 4. Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_recall.append(tr_recall)\n",
    "        val_recall.append(v_recall)\n",
    "        train_f1.append(tr_f1)\n",
    "        val_f1.append(v_f1)\n",
    "        train_precision.append(tr_precision)\n",
    "        val_precision.append(v_precision)\n",
    "        train_preds.append(train_pred)\n",
    "        val_preds.append(val_pred)\n",
    "\n",
    "        train_labels.append(train_label)\n",
    "        val_labels.append(val_label)\n",
    "        \n",
    "        # 5. Save best model based on F1 score and reset patience counter\n",
    "        if v_f1 > best_val_f1:\n",
    "            best_val_f1 = v_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best validation F1: {best_val_f1:.4f}\")\n",
    "            patience_counter = 0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping activated\")\n",
    "                break\n",
    "    \n",
    "    # 6. Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    \n",
    "    # 7. Test\n",
    "    (test_labels, test_preds, test_loss, test_acc,\n",
    "     test_recall, test_f1, test_precision) = validate_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds, labels=[0,1], target_names=['Class 0', 'Class 1'], zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    print(\"\\nConfusion Matrix Test Set:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return model, hf_model, train_loader, val_loader, test_loader, train_losses, val_losses, train_acc, val_acc, test_labels, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c50635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed and run\n",
    "torch.manual_seed(42)\n",
    "trained_model, hf_model, train_loader, val_loader, test_loader, train_losses, val_losses, train_acc, val_acc, test_labels, test_preds = train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68b5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c5287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
