{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aaf33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# September 2025\n",
    "# Train multilingual classification neural network\n",
    "# Violeta Berdejo-Espinola\n",
    "\n",
    "# pytorch dataset\n",
    "# pythorch model\n",
    "# pytorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdfe5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch transformers sys matplotlib numpy sklearn pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23a04b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqvberde/Dropbox/translate/3.classifier_multiling/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system version: 3.11.3 (v3.11.3:f3909b8bc8, Apr  4 2023, 20:12:10) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "pytorch version: 2.6.0\n",
      "numpy version: <module 'numpy.version' from '/Users/uqvberde/Dropbox/translate/3.classifier_multiling/.venv/lib/python3.11/site-packages/numpy/version.py'>\n",
      "mac processor: ('15.7', ('', '', ''), 'arm64')\n",
      "mps is available: True\n",
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('system version:', sys.version)\n",
    "print('pytorch version:', torch.__version__)\n",
    "print('numpy version:', np.version)\n",
    "\n",
    "import platform; \n",
    "\n",
    "print(f'mac processor: {platform.mac_ver()}')\n",
    "print(f'mps is available: {torch.backends.mps.is_built()}')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7226e87",
   "metadata": {},
   "source": [
    "# function to load data and class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869fcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze and visualize class distribution\n",
    "\n",
    "def analyze_class_distribution(labels):\n",
    "   \n",
    "    counter = Counter(labels)\n",
    "    print(\"Class distribution:\")\n",
    "    \n",
    "    for class_id, count in sorted(counter.items()):\n",
    "        \n",
    "        print(f\"Class {class_id}: {count} samples ({count/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    counts = list(counter.values())\n",
    "    imbalance_ratio = max(counts) / min(counts)\n",
    "    \n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376619\n",
      "Class distribution:\n",
      "Class 0: 2965 samples (98.83%)\n",
      "Class 1: 35 samples (1.17%)\n",
      "Imbalance ratio: 84.71:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 2965, 1: 35})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file paths\n",
    "\n",
    "filepath_x = \"../data/for_analysis/eng_x.pickle\"\n",
    "\n",
    "filepath_y = \"../data/for_analysis/eng_y.pickle\"\n",
    "\n",
    "def load_data(path_x, path_y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path_x (list[str]): Input texts.\n",
    "        path_y (list[str]): Input labels.\n",
    "        \n",
    "    Returns:\n",
    "        lists\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path_x,'rb') as x_file:\n",
    "        texts = pickle.load(x_file)\n",
    "    with open (path_y, 'rb') as y_file:\n",
    "        labels = pickle.load(y_file)\n",
    "        \n",
    "    return texts, labels\n",
    "        \n",
    "texts, labels = load_data(\"../data/for_analysis/eng_x.pickle\", \"../data/for_analysis/eng_y.pickle\")\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "# analyze_class_distribution(labels)\n",
    "\n",
    "texts = texts[:3000]\n",
    "labels = labels[:3000]\n",
    "\n",
    "analyze_class_distribution(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0100654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load multilingual data\n",
    "\n",
    "# filepaths_x = [\n",
    "#     \"../data/for_analysis/eng_x.pickle\",\n",
    "    # \"../data/for_analysis/jap_x.pickle\"\n",
    "    # \"../data/for_analysis/spa_x.pickle\"\n",
    "# ]\n",
    "\n",
    "# filepaths_y = [\n",
    "#     \"../data/for_analysis/eng_y.pickle\",\n",
    "    # \"../data/for_analysis/jap_y.pickle\"\n",
    "    # \"../data/for_analysis/spa_y.pickle\"\n",
    "# ]\n",
    "\n",
    "# def load_data(filepaths_x, filepaths_y):\n",
    "#     \"\"\"Load and combine multilingual data\"\"\"\n",
    "#     all_texts = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     for fp_x, fp_y in zip(filepaths_x, filepaths_y):\n",
    "#         with open(fp_x, 'rb') as f:\n",
    "#             texts = pickle.load(f)\n",
    "#         with open(fp_y, 'rb') as f:\n",
    "#             labels = pickle.load(f)\n",
    "            \n",
    "#         all_texts.extend(texts)\n",
    "#         all_labels.extend(labels)\n",
    "    \n",
    "#     return all_texts, all_labels\n",
    "\n",
    "\n",
    "# texts, labels = load_data(filepaths_x, filepaths_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38315b",
   "metadata": {},
   "source": [
    "# encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486f7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hf tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"       # max sequence length of mpnet == 128 ~ 80 words and 768 dimensional vector \n",
    ")                                                                       \n",
    "hf_model = AutoModel.from_pretrained(                                   # use hf Tokenizer and Model function to extend the max length\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "hf_model = hf_model.to(device)\n",
    "\n",
    "# function to encode text\n",
    "\n",
    "def encode_texts(texts, tokenizer, model, device, max_length=512):      # extend the default 128 token limit by setting the max_length to 512\n",
    "    \"\"\"\n",
    "    Encode a list of texts into fixed-size embeddings using mean pooling.\n",
    "    \n",
    "    Args:\n",
    "        texts (list[str]): Input texts.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        model: Multilingual model (encoder).\n",
    "        device (torch.device): \"mpu\".\n",
    "        max_length (int): Maximum sequence length.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,   # because of batch of varying-length texts\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # forward pass (no gradients, eval mode)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "    # mean pooling (ignore padding)\n",
    "    # mask creation\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1).float()  # (batch, seq_len, 1) -> creates a mask of 1s for real tokens and 0s for padding\n",
    "    # weighted sum\n",
    "    sum_embeddings = (token_embeddings * attention_mask).sum(dim=1)  # (batch, hidden_size) -> zeroes out the padding embeddings before summation, preventing from skewing the result\n",
    "    # normalisation calculate the average of the non-padded tokens\n",
    "    sum_mask = attention_mask.sum(dim=1).clamp(min=1e-9)             # (batch, 1)\n",
    "    embeddings = sum_embeddings / sum_mask                           # (batch, hidden_size: 768)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114b83b",
   "metadata": {},
   "source": [
    "# dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a1f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets class\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, model, device, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # create embedding for single text\n",
    "        embedding = encode_texts([text], self.tokenizer, self.model, self.device, self.max_length)\n",
    "        embedding = embedding.squeeze(0)  # remove batch dimension\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3780b",
   "metadata": {},
   "source": [
    "# data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2e1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with imbalance in loss function\n",
    "\n",
    "def compute_loss_weights(labels):\n",
    "    \"\"\"Compute class weights for weighted loss function\"\"\"\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    return torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3c6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stratified minibatches maintaining a 1:1 balanced ratio \n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "# min_batches = min(train_neg_class/batch_sizze train_pos_size/batch_size)\n",
    "\n",
    "class StratifiedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that creates stratified mini-batches where each batch \n",
    "    maintains a 1:1 balanced ratio (for binary classification).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, labels, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels (list or array): Class labels (0 and 1) for all samples\n",
    "            batch_size (int): Total size of mini-batches. Must be even for a perfect 1:1 ratio.\n",
    "            shuffle (bool): Whether to shuffle indices within each class.\n",
    "        \"\"\"\n",
    "        if batch_size % 2 != 0:\n",
    "            raise ValueError(\"batch_size must be an even number to ensure a 1:1 ratio.\")\n",
    "            \n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # 1. Group indices by class\n",
    "        self.class_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            self.class_indices[label].append(idx)\n",
    "        \n",
    "        # Convert to numpy arrays for efficient indexing/shuffling\n",
    "        self.class_indices = {k: np.array(v) for k, v in self.class_indices.items()}\n",
    "        self.classes = sorted(list(self.class_indices.keys())) # Ensure consistent class order (e.g., [0, 1])\n",
    "        \n",
    "        if len(self.classes) != 2:\n",
    "            raise ValueError(\"This sampler is designed for binary classification (2 classes).\")\n",
    "\n",
    "        # 2. 1:1 balanced ratio\n",
    "        samples_per_class_in_batch = self.batch_size // 2\n",
    "        \n",
    "        self.samples_per_class = {\n",
    "            cls: samples_per_class_in_batch\n",
    "            for cls in self.classes\n",
    "        }\n",
    "        \n",
    "        # Determine the limiting factor (the minority class)\n",
    "        self.class_counts = {cls: len(indices) for cls, indices in self.class_indices.items()}\n",
    "        \n",
    "        print(f\"--- Sampler Initialization ---\")\n",
    "        print(f\"Total samples: {self.class_counts}\")\n",
    "        print(f\"Target batch samples (1:1): {self.samples_per_class}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # 3. Shuffle indices within each class if required\n",
    "        if self.shuffle:\n",
    "            class_indices = {\n",
    "                cls: np.random.permutation(indices).tolist()\n",
    "                for cls, indices in self.class_indices.items()\n",
    "            }\n",
    "        else:\n",
    "            class_indices = {cls: indices.tolist() for cls, indices in self.class_indices.items()}\n",
    "            \n",
    "        # 4. Calculate number of full batches limited by the smaller class\n",
    "        # (This avoids errors if one class is exhausted early)\n",
    "        min_batches = min(\n",
    "            len(indices) // self.samples_per_class[cls]\n",
    "            for cls, indices in class_indices.items()\n",
    "        )\n",
    "        \n",
    "        # 5. Create and yield balanced batches\n",
    "        batches = []\n",
    "        for batch_idx in range(min_batches):\n",
    "            batch = []\n",
    "            for cls in self.classes:\n",
    "                samples_to_take = self.samples_per_class[cls]\n",
    "                start_idx = batch_idx * samples_to_take\n",
    "                end_idx = start_idx + samples_to_take\n",
    "                \n",
    "                # Sample the required number of indices from each class's list\n",
    "                batch.extend(class_indices[cls][start_idx:end_idx])\n",
    "            \n",
    "            # Shuffle indices within the batch to mix the classes\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(batch)\n",
    "                \n",
    "            batches.append(batch)\n",
    "        \n",
    "        # 6. Shuffle the order of the batches\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batches)\n",
    "            \n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Calculate the number of full, balanced batches that can be created\n",
    "        min_batches = min(\n",
    "            len(indices) // self.samples_per_class[cls]\n",
    "            for cls, indices in self.class_indices.items()\n",
    "        )\n",
    "        return min_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a8c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create stratified minibatches tha maintain the same class distriburion --> use this instead of 1:1 ratio minibatches\n",
    "\n",
    "# from torch.utils.data import Sampler\n",
    "# from collections import defaultdict\n",
    "\n",
    "# class StratifiedBatchSampler(Sampler):\n",
    "#     \"\"\"\n",
    "#     Sampler that creates stratified mini-batches \n",
    "#     that maintain the same class distribution\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, labels, batch_size, shuffle=True):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             labels (list or array): Class labels for all samples\n",
    "#             batch_size (int): Size of mini-batches\n",
    "#             shuffle (bool): Whether to shuffle within each class\n",
    "#         \"\"\"\n",
    "#         self.labels = np.array(labels)\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "        \n",
    "#         # group indices by class\n",
    "#         self.class_indices = defaultdict(list)\n",
    "#         for idx, label in enumerate(self.labels):\n",
    "#             self.class_indices[label].append(idx)\n",
    "        \n",
    "#         # convert to lists\n",
    "#         self.class_indices = {k: np.array(v) for k, v in self.class_indices.items()}\n",
    "#         self.classes = list(self.class_indices.keys())\n",
    "        \n",
    "#         # calculate samples per class per batch\n",
    "#         self.class_counts = {cls: len(indices) for cls, indices in self.class_indices.items()}\n",
    "#         total_samples = sum(self.class_counts.values())\n",
    "        \n",
    "#         # Proportional representation in each batch\n",
    "#         self.samples_per_class = {\n",
    "#             cls: max(1, int(batch_size * count / total_samples))\n",
    "#             for cls, count in self.class_counts.items()\n",
    "#         }\n",
    "        \n",
    "#         # Adjust to ensure batch_size is maintained\n",
    "#         diff = batch_size - sum(self.samples_per_class.values())\n",
    "#         if diff != 0:\n",
    "#             # Add/remove from largest class\n",
    "#             largest_class = max(self.class_counts, key=self.class_counts.get)\n",
    "#             self.samples_per_class[largest_class] += diff\n",
    "        \n",
    "#         print(f\"Samples per class per batch: {self.samples_per_class}\")\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         # Shuffle indices within each class if required\n",
    "#         if self.shuffle:\n",
    "#             class_indices = {\n",
    "#                 cls: np.random.permutation(indices).tolist()\n",
    "#                 for cls, indices in self.class_indices.items()\n",
    "#             }\n",
    "#         else:\n",
    "#             class_indices = {cls: indices.tolist() for cls, indices in self.class_indices.items()}\n",
    "        \n",
    "#         # Create batches\n",
    "#         batches = []\n",
    "        \n",
    "#         # Calculate number of batches\n",
    "#         min_batches = min(\n",
    "#             len(indices) // self.samples_per_class[cls]\n",
    "#             for cls, indices in class_indices.items()\n",
    "#         )\n",
    "        \n",
    "#         for batch_idx in range(min_batches):\n",
    "#             batch = []\n",
    "#             for cls in self.classes:\n",
    "#                 start_idx = batch_idx * self.samples_per_class[cls]\n",
    "#                 end_idx = start_idx + self.samples_per_class[cls]\n",
    "#                 batch.extend(class_indices[cls][start_idx:end_idx])\n",
    "            \n",
    "#             # Shuffle within batch\n",
    "#             if self.shuffle:\n",
    "#                 np.random.shuffle(batch)\n",
    "            \n",
    "#             batches.append(batch)\n",
    "        \n",
    "#         # Handle remaining samples\n",
    "#         remaining = []\n",
    "#         for cls in self.classes:\n",
    "#             start_idx = min_batches * self.samples_per_class[cls]\n",
    "#             remaining.extend(class_indices[cls][start_idx:])\n",
    "        \n",
    "#         # Create final partial batch if we have remaining samples\n",
    "#         if len(remaining) >= self.batch_size // 2:  # Only if we have substantial samples\n",
    "#             if self.shuffle:\n",
    "#                 np.random.shuffle(remaining)\n",
    "#             batches.append(remaining[:self.batch_size])\n",
    "        \n",
    "#         # Shuffle batch order\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(batches)\n",
    "        \n",
    "#         for batch in batches:\n",
    "#             yield batch\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         min_batches = min(\n",
    "#             len(indices) // self.samples_per_class[cls]\n",
    "#             for cls, indices in self.class_indices.items()\n",
    "#         )\n",
    "#         return min_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee0a4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weighted random sampler --> use instead of mini batches\n",
    "\n",
    "# def weighted_sampler(labels):\n",
    "#     \"\"\"Assign a weight inversely proportional to its class frequency\"\"\"\n",
    "    \n",
    "#     # 1. Create mapping of each class to a weight equal to 1 / count\n",
    "#     class_counts = Counter(labels)\n",
    "#     class_weights = {cls: 1.0/count for cls, count in class_counts.items()}\n",
    "#     print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "#     # 2. Replace class with that weight.\n",
    "#     sample_weights = [class_weights[label] for label in labels]\n",
    "#     print(len(sample_weights))\n",
    "    \n",
    "#     # 3. Sampler object\n",
    "#     sampler = WeightedRandomSampler(\n",
    "#         weights=sample_weights,\n",
    "#         num_samples=len(sample_weights),\n",
    "#         replacement=True # samples can be picked multiple times per epoch\n",
    "#     )\n",
    "    \n",
    "#     return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597e173",
   "metadata": {},
   "source": [
    "# classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308f242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier class\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=None, hidden_dims=[512, 512, 512], num_classes=2, dropout=0.2): \n",
    "        super(MLPClassifier, self).__init__()                                               \n",
    "        \"\"\"\n",
    "        Multi-layer perceptron for text classification.\n",
    "        \n",
    "        Args:\n",
    "        input_dim (feature vector): per sample and outputs\n",
    "        hidden_dim (list): hidden state\n",
    "        num_classes (int): classification classes\n",
    "        dropout (float): fraction of neurons to drop during training at each training step\n",
    "        \n",
    "        Returns: num_classes logits per sample\n",
    "        \"\"\"\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                \n",
    "                # a single fully connected layer -> performs the linear transformation on input data: 768 to 512\n",
    "                nn.Linear(prev_dim, hidden_dim), # it applies the operation: y = xW^T + b\n",
    "                                                 \n",
    "                # nn.BatchNorm1d(hidden_dim), # normalize inputs  - if dropout is used normalization is not \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        # container that organizes multiple layers into a pipeline\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1dab0",
   "metadata": {},
   "source": [
    "# train eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e64782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for embeddings, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass (no gradients)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass (update gradients)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Collect predictions and labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_tr = 100 * correct / total\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    recall_tr = recall_score(all_labels, all_preds)\n",
    "    f1_tr = f1_score(all_labels, all_preds)\n",
    "    precision_tr = precision_score(all_labels, all_preds)\n",
    "    \n",
    "    return (all_preds, all_labels, \n",
    "            avg_loss, recall_tr, precision_tr, f1_tr, accuracy_tr)\n",
    "\n",
    "# eval function \n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 3. Collect predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy_val = 100 * correct / total\n",
    "    \n",
    "    # 5. Calculate metrics for each class\n",
    "    recall_val = recall_score(all_labels, all_preds)\n",
    "    f1_val = f1_score(all_labels, all_preds)\n",
    "    precision_val = precision_score(all_labels, all_preds)\n",
    "    \n",
    "    return (all_preds, all_labels, \n",
    "            avg_loss, recall_val, precision_val, f1_val, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bcef7",
   "metadata": {},
   "source": [
    "# load data, split, create datastes and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "643e0020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 3000\n",
      "Class distribution:\n",
      "Class 0: 2965 samples (98.83%)\n",
      "Class 1: 35 samples (1.17%)\n",
      "Imbalance ratio: 84.71:1\n",
      "Train: 2100,       Val: 720,       Test: 180\n",
      "--- Sampler Initialization ---\n",
      "Total samples: {0: 2075, 1: 25}\n",
      "Target batch samples (1:1): {0: 16, 1: 16}\n",
      "embedding dimension: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 23, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load data\n",
    "\n",
    "# texts, labels = load_data(filepath_x, filepath_y)\n",
    "\n",
    "print(f\"Total documents: {len(texts)}\")\n",
    "\n",
    "class_distribution = analyze_class_distribution(labels)\n",
    "\n",
    "weights = compute_loss_weights(labels)\n",
    "\n",
    "# split data       %% run it with different seeds %%\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=seed, stratify=labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=seed, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, \\\n",
    "      Val: {len(X_val)}, \\\n",
    "      Test: {len(X_test)}\")\n",
    "    \n",
    "# datasets\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, hf_model, device)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, hf_model, device)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer, hf_model, device)\n",
    "\n",
    "# weighted sampler\n",
    "\n",
    "# train_sampler, class_weights = weighted_sampler(y_train)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_batch_sampler = StratifiedBatchSampler(\n",
    "    labels=y_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_batch_sampler, num_workers=0)  # batch_sampler\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# get embedding dimension\n",
    "\n",
    "sample_embedding, _ = train_dataset[0]\n",
    "embedding_dim = sample_embedding.shape[0]\n",
    "\n",
    "print(f\"embedding dimension: {embedding_dim}\")\n",
    "# print(f\"sample embedding: {sample_embedding[0]}\")\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166ffe3",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ea0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_loop():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MLPClassifier(input_dim=embedding_dim).to(device)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    \n",
    "    num_epochs = 2                       \n",
    "    patience = 5  # stop if no improvement after 5 epochs\n",
    "    patience_counter = 0\n",
    "    best_val_f1 = 0 \n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.05)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    train_preds, train_labels = [],[]\n",
    "    val_preds, val_labels = [],[]\n",
    "    train_losses, val_losses = [],[]\n",
    "    train_accs, val_accs = [],[]\n",
    "    train_recall, val_recall = [],[]\n",
    "    train_f1, val_f1 = [],[]\n",
    "    train_precision, val_precision = [],[]\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # 1. Train\n",
    "        (train_pred, train_label, train_loss, tr_recall, tr_precision, tr_f1, train_acc) = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # 2. Validate\n",
    "        (val_pred, val_label, val_loss, v_recall, v_f1, v_precision, val_acc) = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Train Recall: {tr_recall:.4f}\")\n",
    "        print(f\"Train F1: {tr_f1:.4f}\")\n",
    "        print(f\"Train Precision: {tr_precision:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Val Recall: {v_recall:.4f}\")\n",
    "        print(f\"Val F1: {v_f1:.4f}\")\n",
    "        print(f\"Val Precision: {v_precision:.4f}\")\n",
    "        \n",
    "        # 3. Update learning rate\n",
    "        optimizer.step() # scheduler\n",
    "        \n",
    "        # 4. Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_recall.append(tr_recall)\n",
    "        val_recall.append(v_recall)\n",
    "        train_f1.append(tr_f1)\n",
    "        val_f1.append(v_f1)\n",
    "        train_precision.append(tr_precision)\n",
    "        val_precision.append(v_precision)\n",
    "        train_preds.append(train_pred)\n",
    "        val_preds.append(val_pred)\n",
    "\n",
    "        train_labels.append(train_label)\n",
    "        val_labels.append(val_label)\n",
    "        \n",
    "        # 5. Save best model based on F1 score and reset patience counter\n",
    "        if v_f1 > best_val_f1:\n",
    "            best_val_f1 = v_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best validation F1: {best_val_f1:.4f}\")\n",
    "            patience_counter = 0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping activated\")\n",
    "                break\n",
    "    \n",
    "    # 6. Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    \n",
    "    # 7. Test\n",
    "    (test_labels, test_preds, test_loss, test_acc,\n",
    "     test_recall, test_f1, test_precision) = validate_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds, labels=[0,1], target_names=['Class 0', 'Class 1'], zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    print(\"\\nConfusion Matrix Test Set:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return model, hf_model, train_loader, val_loader, test_loader, train_losses, val_losses, train_acc, val_acc, test_labels, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c50635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 920,066\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6799, Train Acc: 50.00%\n",
      "Train Recall: 1.0000\n",
      "Train F1: 0.6667\n",
      "Train Precision: 0.5000\n",
      "Val Loss: 7.5208, Val Acc: 1.11%\n",
      "Val Recall: 1.0000\n",
      "Val F1: 0.0111\n",
      "Val Precision: 0.0220\n",
      "New best validation F1: 0.0111\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:18<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0532, Train Acc: 50.00%\n",
      "Train Recall: 1.0000\n",
      "Train F1: 0.6667\n",
      "Train Precision: 0.5000\n",
      "Val Loss: 15.6713, Val Acc: 1.11%\n",
      "Val Recall: 1.0000\n",
      "Val F1: 0.0111\n",
      "Val Precision: 0.0220\n",
      "No improvement. Early stopping counter: 1/5\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:08<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Test Loss: 69.2760\n",
      "Test Accuracy: 1.00%\n",
      "Test Recall: 0.0111\n",
      "Test F1: 0.0220\n",
      "Test Precision: 1.1111\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         0\n",
      "     Class 1       1.00      0.01      0.02       180\n",
      "\n",
      "    accuracy                           0.01       180\n",
      "   macro avg       0.50      0.01      0.01       180\n",
      "weighted avg       1.00      0.01      0.02       180\n",
      "\n",
      "\n",
      "Confusion Matrix Test Set:\n",
      "[[  0   0]\n",
      " [178   2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# seed and run\n",
    "torch.manual_seed(42)\n",
    "trained_model, hf_model, train_loader, val_loader, test_loader, train_losses, val_losses, train_acc, val_acc, test_labels, test_preds = train_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
